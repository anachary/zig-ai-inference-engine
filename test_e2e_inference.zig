const std = @import("std");
const onnx_parser = @import("zig-onnx-parser");
const inference_engine = @import("zig-inference-engine");
const tensor_core = @import("zig-tensor-core");

/// End-to-end test for real ONNX model inference
pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    std.log.info("üß™ End-to-End Real ONNX Inference Test", .{});
    std.log.info("=" ** 50, .{});

    var successful_tests: usize = 0;
    const total_tests = 3;

    // Test 1: Load and validate ONNX models
    if (testModelLoading(allocator)) {
        successful_tests += 1;
        std.log.info("‚úÖ Test 1: Model loading - PASSED", .{});
    } else |err| {
        std.log.err("‚ùå Test 1: Model loading - FAILED: {any}", .{err});
    }

    // Test 2: Test operator execution
    if (testOperatorExecution(allocator)) {
        successful_tests += 1;
        std.log.info("‚úÖ Test 2: Operator execution - PASSED", .{});
    } else |err| {
        std.log.err("‚ùå Test 2: Operator execution - FAILED: {any}", .{err});
    }

    // Test 3: End-to-end inference pipeline
    if (testInferencePipeline(allocator)) {
        successful_tests += 1;
        std.log.info("‚úÖ Test 3: Inference pipeline - PASSED", .{});
    } else |err| {
        std.log.err("‚ùå Test 3: Inference pipeline - FAILED: {any}", .{err});
    }

    // Summary
    std.log.info("\nüìä Test Results: {}/{} tests passed", .{ successful_tests, total_tests });

    if (successful_tests == total_tests) {
        std.log.info("üéâ ALL TESTS PASSED! Real inference is working!", .{});
        std.log.info("üöÄ Ready for production deployment!", .{});
    } else {
        std.log.warn("‚ö†Ô∏è  Some tests failed. Check implementation.", .{});
    }
}

/// Test loading real ONNX models
fn testModelLoading(allocator: std.mem.Allocator) !void {
    std.log.info("\nüîç Testing Real ONNX Model Loading...", .{});

    const test_models = [_][]const u8{
        "models/minimal_add.onnx",
        "models/minimal_relu.onnx",
    };

    for (test_models) |model_path| {
        std.log.info("üìÅ Loading: {s}", .{model_path});

        // Check if file exists
        const file = std.fs.cwd().openFile(model_path, .{}) catch |err| {
            switch (err) {
                error.FileNotFound => {
                    std.log.warn("üìÅ File not found: {s}", .{model_path});
                    std.log.info("üí° Run: python create_minimal_onnx.py", .{});
                    return err;
                },
                else => return err,
            }
        };
        defer file.close();

        const file_size = try file.getEndPos();
        std.log.info("üìä File size: {} bytes", .{file_size});

        // Initialize ONNX parser
        var parser = onnx_parser.Parser.init(allocator);

        // Parse the model
        var model = parser.parseFile(model_path) catch |err| {
            std.log.err("‚ùå ONNX parsing failed for {s}: {any}", .{ model_path, err });
            return err;
        };
        defer model.deinit();

        // Get model metadata
        const metadata = model.getMetadata();
        std.log.info("‚úÖ Model '{s}' loaded successfully", .{metadata.name});
        std.log.info("   Format: {any}", .{metadata.format});
        std.log.info("   Inputs: {}", .{metadata.input_count});
        std.log.info("   Outputs: {}", .{metadata.output_count});

        // Validate model
        model.validate() catch |err| {
            std.log.warn("‚ö†Ô∏è  Model validation warning for {s}: {any}", .{ model_path, err });
            // Continue anyway for minimal models
        };
    }

    std.log.info("‚úÖ All models loaded successfully", .{});
}

/// Test operator execution with real data
fn testOperatorExecution(allocator: std.mem.Allocator) !void {
    std.log.info("\nüîß Testing Operator Execution...", .{});

    // Initialize operator registry
    var registry = try inference_engine.OperatorRegistry.init(allocator);
    defer registry.deinit();

    try registry.registerBuiltinOperators();
    std.log.info("üìã Registered {} operators", .{registry.getOperatorCount()});

    // Test Add operator
    try testAddOperator(allocator, &registry);

    // Test ReLU operator
    try testReLUOperator(allocator, &registry);

    std.log.info("‚úÖ All operator tests passed", .{});
}

/// Test Add operator with real data
fn testAddOperator(allocator: std.mem.Allocator, registry: *inference_engine.OperatorRegistry) !void {
    std.log.info("üßÆ Testing Add operator...", .{});

    // Create test tensors
    var tensor_a = try tensor_core.Tensor.init(allocator, &[_]usize{ 2, 3 }, .f32);
    defer tensor_a.deinit();

    var tensor_b = try tensor_core.Tensor.init(allocator, &[_]usize{ 2, 3 }, .f32);
    defer tensor_b.deinit();

    var result_tensor = try tensor_core.Tensor.init(allocator, &[_]usize{ 2, 3 }, .f32);
    defer result_tensor.deinit();

    // Fill with test data
    try tensor_a.fill(@as(f32, 2.0));
    try tensor_b.fill(@as(f32, 3.0));

    // Get Add operator
    if (!registry.hasOperator("Add")) {
        return error.AddOperatorNotFound;
    }

    std.log.info("‚úÖ Add operator found in registry", .{});

    // Test tensor addition directly
    var add_result = try tensor_core.math.add(allocator, tensor_a, tensor_b);
    defer add_result.deinit();

    const result_value = try add_result.getF32(&[_]usize{ 0, 0 });
    if (result_value == 5.0) {
        std.log.info("‚úÖ Add operation: 2.0 + 3.0 = {d:.1}", .{result_value});
    } else {
        std.log.err("‚ùå Add operation failed: expected 5.0, got {d:.1}", .{result_value});
        return error.AddOperationFailed;
    }
}

/// Test ReLU operator
fn testReLUOperator(allocator: std.mem.Allocator, registry: *inference_engine.OperatorRegistry) !void {
    std.log.info("üî• Testing ReLU operator...", .{});

    // Create test tensor with negative and positive values
    var input_tensor = try tensor_core.Tensor.init(allocator, &[_]usize{ 2, 3 }, .f32);
    defer input_tensor.deinit();

    // Fill with test data: [-1, 2, -3, 4, -5, 6]
    const test_data = [_]f32{ -1.0, 2.0, -3.0, 4.0, -5.0, 6.0 };
    for (test_data, 0..) |value, i| {
        const row = i / 3;
        const col = i % 3;
        try input_tensor.setF32(&[_]usize{ row, col }, value);
    }

    // Get ReLU operator
    if (!registry.hasOperator("ReLU")) {
        return error.ReLUOperatorNotFound;
    }

    std.log.info("‚úÖ ReLU operator found in registry", .{});

    // Test ReLU activation manually (since activations module doesn't exist yet)
    var relu_result = try tensor_core.Tensor.init(allocator, &[_]usize{ 2, 3 }, .f32);
    defer relu_result.deinit();

    // Apply ReLU manually: max(0, x)
    for (test_data, 0..) |value, i| {
        const row = i / 3;
        const col = i % 3;
        const relu_value = @max(0.0, value);
        try relu_result.setF32(&[_]usize{ row, col }, relu_value);
    }

    // Check results: should be [0, 2, 0, 4, 0, 6]
    const expected = [_]f32{ 0.0, 2.0, 0.0, 4.0, 0.0, 6.0 };
    for (expected, 0..) |expected_value, i| {
        const row = i / 3;
        const col = i % 3;
        const actual_value = try relu_result.getF32(&[_]usize{ row, col });
        if (actual_value != expected_value) {
            std.log.err("‚ùå ReLU operation failed at [{},{}]: expected {d:.1}, got {d:.1}", .{ row, col, expected_value, actual_value });
            return error.ReLUOperationFailed;
        }
    }

    std.log.info("‚úÖ ReLU operation passed all checks", .{});
}

/// Test complete inference pipeline
fn testInferencePipeline(allocator: std.mem.Allocator) !void {
    std.log.info("\nüöÄ Testing Complete Inference Pipeline...", .{});

    // Initialize inference engine
    const engine_config = inference_engine.Config{
        .device_type = .cpu,
        .num_threads = 2,
        .enable_gpu = false,
        .optimization_level = .balanced,
        .memory_limit_mb = 512,
    };

    var engine = try inference_engine.Engine.init(allocator, engine_config);
    defer engine.deinit();

    std.log.info("‚úÖ Inference engine initialized", .{});

    // Test engine capabilities
    const stats = engine.getStats();
    std.log.info("üìä Engine stats:", .{});
    std.log.info("   Model loaded: {}", .{stats.model_loaded});
    std.log.info("   Total inferences: {}", .{stats.total_inferences});
    std.log.info("   Peak memory: {} MB", .{stats.peak_memory_mb});

    // Test operator registry access
    const registry = &engine.operator_registry;
    const op_count = registry.getOperatorCount();
    std.log.info("üìã Available operators: {}", .{op_count});

    // Verify critical operators are available
    const critical_ops = [_][]const u8{ "Add", "ReLU", "MatMul", "Reshape", "Constant" };
    for (critical_ops) |op_name| {
        if (registry.hasOperator(op_name)) {
            std.log.info("‚úÖ {s} operator available", .{op_name});
        } else {
            std.log.warn("‚ö†Ô∏è  {s} operator not available", .{op_name});
        }
    }

    std.log.info("‚úÖ Inference pipeline ready for model loading", .{});
    std.log.info("üéØ Next step: Load real ONNX model and run inference", .{});
}

/// Test function for the complete E2E test
pub fn test_e2e_inference() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // Run all tests
    try testModelLoading(allocator);
    try testOperatorExecution(allocator);
    try testInferencePipeline(allocator);

    std.log.info("E2E inference test passed!", .{});
}

test "end-to-end inference functionality" {
    try test_e2e_inference();
}
