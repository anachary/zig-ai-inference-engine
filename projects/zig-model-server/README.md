# Zig Model Server

üåê **HTTP API and CLI interfaces for neural network model serving**

A focused library following the **Single Responsibility Principle** - handles only HTTP API, CLI interfaces, model management, and serving infrastructure.

## üéØ Single Responsibility

This project has **one clear purpose**: Provide HTTP API and CLI interfaces for neural network model serving and management.

**What it does:**
- ‚úÖ HTTP server with RESTful API endpoints
- ‚úÖ Comprehensive CLI interface for model management
- ‚úÖ Model loading, unloading, and metadata management
- ‚úÖ Real-time inference API with JSON responses
- ‚úÖ Interactive chat interface for conversational AI
- ‚úÖ Health checks, metrics, and monitoring endpoints
- ‚úÖ WebSocket support for streaming responses

**What it doesn't do:**
- ‚ùå Model execution (use zig-inference-engine)
- ‚ùå Tensor operations (use zig-tensor-core)
- ‚ùå Model parsing (use zig-onnx-parser)
- ‚ùå Low-level inference (use zig-inference-engine)

## üöÄ Quick Start

### Installation
```bash
# Add as dependency in your build.zig
const model_server = b.dependency("zig-model-server", .{
    .target = target,
    .optimize = optimize,
});
```

### HTTP Server Usage
```zig
const std = @import("std");
const model_server = @import("zig-model-server");
const inference_engine = @import("zig-inference-engine");
const onnx_parser = @import("zig-onnx-parser");

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // Initialize inference engine
    var engine = try inference_engine.createServerEngine(allocator);
    defer engine.deinit();

    // Initialize model server
    const server_config = model_server.ServerConfig{
        .host = "0.0.0.0",
        .port = 8080,
        .max_connections = 100,
        .enable_cors = true,
        .enable_metrics = true,
    };

    var server = try model_server.HTTPServer.init(allocator, server_config);
    defer server.deinit();

    // Attach inference engine
    try server.attachInferenceEngine(&engine);

    // Load a model
    try server.loadModel("my-model", "path/to/model.onnx");

    // Start server
    std.log.info("Starting model server on http://{}:{}", .{ server_config.host, server_config.port });
    try server.start();
}
```

### CLI Usage
```bash
# Start server
zig-model-server serve --host 0.0.0.0 --port 8080

# Load a model
zig-model-server load-model --name my-model --path model.onnx

# Run inference
zig-model-server infer --model my-model --input input.json

# List models
zig-model-server list-models

# Get model info
zig-model-server model-info --name my-model

# Health check
zig-model-server health

# Interactive chat
zig-model-server chat --model my-chat-model
```

## üìö API Reference

### HTTP Endpoints

#### Model Management
```http
# Load a model
POST /api/v1/models
Content-Type: application/json
{
  "name": "my-model",
  "path": "/path/to/model.onnx",
  "config": {
    "max_batch_size": 4,
    "optimization_level": "balanced"
  }
}

# List models
GET /api/v1/models

# Get model info
GET /api/v1/models/{model_name}

# Unload model
DELETE /api/v1/models/{model_name}
```

#### Inference
```http
# Run inference
POST /api/v1/models/{model_name}/infer
Content-Type: application/json
{
  "inputs": [
    {
      "name": "input",
      "shape": [1, 3, 224, 224],
      "data": [0.1, 0.2, 0.3, ...]
    }
  ]
}

# Batch inference
POST /api/v1/models/{model_name}/infer/batch
Content-Type: application/json
{
  "batch_inputs": [
    {"inputs": [...]},
    {"inputs": [...]}
  ]
}
```

#### Chat Interface
```http
# Chat completion
POST /api/v1/chat/completions
Content-Type: application/json
{
  "model": "my-chat-model",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "stream": false
}

# Streaming chat
POST /api/v1/chat/completions
Content-Type: application/json
{
  "model": "my-chat-model",
  "messages": [
    {"role": "user", "content": "Tell me a story"}
  ],
  "stream": true
}
```

#### Health and Metrics
```http
# Health check
GET /health

# Metrics
GET /metrics

# Server info
GET /api/v1/info
```

### Core Types
```zig
// HTTP Server
const HTTPServer = struct {
    pub fn init(allocator: Allocator, config: ServerConfig) !HTTPServer
    pub fn deinit(self: *HTTPServer) void
    pub fn attachInferenceEngine(self: *HTTPServer, engine: *inference_engine.Engine) !void
    pub fn loadModel(self: *HTTPServer, name: []const u8, path: []const u8) !void
    pub fn unloadModel(self: *HTTPServer, name: []const u8) !void
    pub fn start(self: *HTTPServer) !void
    pub fn stop(self: *HTTPServer) void
};

// CLI Interface
const CLI = struct {
    pub fn init(allocator: Allocator) CLI
    pub fn run(self: *CLI, args: []const []const u8) !void
    pub fn printHelp(self: *const CLI) void
};

// Model Manager
const ModelManager = struct {
    pub fn init(allocator: Allocator, engine: *inference_engine.Engine) !ModelManager
    pub fn loadModel(self: *ModelManager, name: []const u8, path: []const u8, config: ModelConfig) !void
    pub fn unloadModel(self: *ModelManager, name: []const u8) !void
    pub fn getModel(self: *const ModelManager, name: []const u8) ?*LoadedModel
    pub fn listModels(self: *const ModelManager) []ModelInfo
};

// Configuration
const ServerConfig = struct {
    host: []const u8 = "127.0.0.1",
    port: u16 = 8080,
    max_connections: u32 = 100,
    request_timeout_ms: u32 = 30000,
    enable_cors: bool = true,
    enable_metrics: bool = true,
    enable_websockets: bool = true,
    static_files_dir: ?[]const u8 = null,
    tls_cert_path: ?[]const u8 = null,
    tls_key_path: ?[]const u8 = null,
};
```

## üèóÔ∏è Architecture

### Design Principles
1. **Single Responsibility**: Only HTTP API and CLI interfaces
2. **RESTful Design**: Clean, predictable API endpoints
3. **Async/Await**: Non-blocking request handling
4. **Middleware Support**: Extensible request/response pipeline
5. **Error Handling**: Comprehensive error responses with proper HTTP status codes

### Request Pipeline
1. **Request Parsing**: HTTP request parsing and validation
2. **Authentication**: Optional API key or token validation
3. **Rate Limiting**: Request throttling and quota management
4. **Routing**: URL pattern matching and handler dispatch
5. **Model Management**: Model loading/unloading coordination
6. **Inference Execution**: Delegation to zig-inference-engine
7. **Response Formatting**: JSON response serialization
8. **Logging**: Request/response logging and metrics

### WebSocket Support
- **Real-time Inference**: Streaming inference results
- **Chat Interface**: Interactive conversational AI
- **Model Events**: Real-time model loading/unloading notifications
- **Metrics Streaming**: Live performance metrics

## üß™ Testing

```bash
# Run all tests
zig build test

# Run specific tests
zig build test -- --filter "http"
zig build test -- --filter "cli"
zig build test -- --filter "models"

# Integration tests
zig build test-integration

# Load tests
zig build test-load
```

## üìä Performance

### Benchmarks (on modern desktop)
- **Request Latency**: < 1ms (without inference)
- **Throughput**: > 10,000 requests/second
- **Memory Usage**: < 50MB base + model memory
- **Concurrent Connections**: 1,000+ simultaneous
- **Model Loading**: < 5 seconds for typical models

### Optimization Features
- **Connection Pooling**: Efficient connection reuse
- **Request Batching**: Automatic batching for efficiency
- **Response Caching**: Optional response caching
- **Compression**: Gzip/deflate response compression
- **Keep-Alive**: HTTP keep-alive support

## üéØ Use Cases

### Perfect For
- **Model Serving**: Production model deployment
- **API Development**: Building AI-powered applications
- **Microservices**: AI inference microservice
- **Development**: Local model testing and development
- **Chat Applications**: Conversational AI interfaces

### Integration Examples
```zig
// With custom middleware
const auth_middleware = @import("auth_middleware");
try server.addMiddleware(auth_middleware.authenticate);

// With custom routes
try server.addRoute("GET", "/custom", customHandler);

// With WebSocket handlers
try server.addWebSocketHandler("/ws/chat", chatWebSocketHandler);

// With metrics collection
const metrics = try server.getMetrics();
std.log.info("Requests served: {}", .{metrics.total_requests});
```

## üìà Roadmap

### Current: v0.1.0
- ‚úÖ Basic HTTP server
- ‚úÖ RESTful API endpoints
- ‚úÖ CLI interface
- ‚úÖ Model management

### Next: v0.2.0
- üîÑ WebSocket support
- üîÑ Streaming responses
- üîÑ Authentication
- üîÑ Rate limiting

### Future: v1.0.0
- ‚è≥ Load balancing
- ‚è≥ Horizontal scaling
- ‚è≥ Advanced monitoring
- ‚è≥ Plugin system

## ü§ù Contributing

This project follows strict **Single Responsibility Principle**:

**‚úÖ Contributions Welcome:**
- HTTP API improvements
- CLI feature enhancements
- Model management features
- Performance optimizations
- Documentation improvements

**‚ùå Out of Scope:**
- Model execution (belongs in inference-engine)
- Tensor operations (belongs in tensor-core)
- Model parsing (belongs in onnx-parser)
- Low-level inference (belongs in inference-engine)

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

---

**Part of the Zig AI Ecosystem:**
- üßÆ [zig-tensor-core](../zig-tensor-core) - Tensor operations
- üì¶ [zig-onnx-parser](../zig-onnx-parser) - Model parsing
- ‚öôÔ∏è [zig-inference-engine](../zig-inference-engine) - Model execution  
- üåê **zig-model-server** (this project) - HTTP API & CLI
- üéØ [zig-ai-platform](../zig-ai-platform) - Unified orchestrator
