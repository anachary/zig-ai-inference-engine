# üöÄ Massive Pretrained LLM Deployment on Azure Kubernetes Service (AKS)

## üìã Table of Contents

1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Azure Infrastructure Setup](#azure-infrastructure-setup)
4. [Model Preparation](#model-preparation)
5. [AKS Cluster Configuration](#aks-cluster-configuration)
6. [Deployment Steps](#deployment-steps)
7. [Scaling and Optimization](#scaling-and-optimization)
8. [Monitoring and Troubleshooting](#monitoring-and-troubleshooting)
9. [Cost Optimization](#cost-optimization)
10. [Production Considerations](#production-considerations)

## üéØ Overview

This guide provides comprehensive instructions for deploying massive pretrained language models (175B+ parameters) on Azure Kubernetes Service using the Zig AI distributed inference platform. The system supports horizontal model sharding across multiple GPU nodes for efficient inference at scale.

### Supported Models
- **GPT-3/GPT-4 class models** (175B+ parameters)
- **LLaMA/LLaMA-2** (70B+ parameters)
- **PaLM** (540B+ parameters)
- **Custom transformer models** with ONNX/SafeTensors format

### Architecture Benefits
- ‚úÖ **Horizontal Model Sharding**: Split large models across multiple nodes
- ‚úÖ **Auto-scaling**: Dynamic scaling based on inference demand
- ‚úÖ **Fault Tolerance**: Automatic failover and recovery
- ‚úÖ **Cost Optimization**: Spot instances and burst scaling
- ‚úÖ **GPU Optimization**: Efficient GPU memory management

## üîß Prerequisites

### Required Tools
```bash
# Install Azure CLI
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Install Helm
curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update && sudo apt-get install helm

# Install Docker
sudo apt-get update && sudo apt-get install docker.io
sudo usermod -aG docker $USER
```

### Azure Subscription Requirements
- **Subscription**: Azure subscription with sufficient quota
- **GPU Quota**: Request quota increase for GPU VMs (NC/ND/NV series)
- **Storage**: Premium storage for model files (1-10TB depending on model size)
- **Networking**: Virtual network with sufficient IP address space

### Model Requirements
- **Format**: ONNX, SafeTensors, or PyTorch format
- **Size**: 70B+ parameters (350GB+ model files)
- **Sharding**: Pre-sharded or automatic sharding support
- **Quantization**: Optional INT8/FP16 quantization for memory optimization

## üèóÔ∏è Azure Infrastructure Setup

### Step 1: Create Resource Group and Basic Resources

```bash
# Set environment variables
export RESOURCE_GROUP="zig-ai-llm-rg"
export LOCATION="eastus"
export ACR_NAME="zigaillmregistry"
export AKS_CLUSTER_NAME="zig-ai-llm-cluster"
export STORAGE_ACCOUNT="zigaillmstorage"

# Login to Azure
az login

# Create resource group
az group create --name $RESOURCE_GROUP --location $LOCATION

# Create Azure Container Registry
az acr create \
  --resource-group $RESOURCE_GROUP \
  --name $ACR_NAME \
  --sku Premium \
  --admin-enabled true

# Create storage account for models
az storage account create \
  --name $STORAGE_ACCOUNT \
  --resource-group $RESOURCE_GROUP \
  --location $LOCATION \
  --sku Premium_LRS \
  --kind StorageV2
```

### Step 2: Request GPU Quota (Critical Step)

```bash
# Check current quota
az vm list-usage --location $LOCATION --query "[?contains(name.value, 'NC')]" --output table

# Request quota increase (via Azure Portal or Support Ticket)
# Required quotas for massive LLMs:
# - Standard NC24s v3 Family vCPUs: 96+ (for 4 nodes)
# - Standard ND40rs v2 Family vCPUs: 160+ (for 4 nodes)
# - Premium Storage: 50TB+
```

### Step 3: Create Virtual Network

```bash
# Create virtual network
az network vnet create \
  --resource-group $RESOURCE_GROUP \
  --name zig-ai-vnet \
  --address-prefixes 10.0.0.0/8 \
  --subnet-name aks-subnet \
  --subnet-prefixes 10.240.0.0/16

# Create additional subnet for services
az network vnet subnet create \
  --resource-group $RESOURCE_GROUP \
  --vnet-name zig-ai-vnet \
  --name services-subnet \
  --address-prefixes 10.241.0.0/16
```

## üì¶ Model Preparation

### Step 1: Download and Prepare Model Files

```bash
# Create model directory
mkdir -p ./models/gpt3-175b

# Example: Download LLaMA-2 70B model (replace with your model)
# Note: Ensure you have proper licensing and access rights
git lfs clone https://huggingface.co/meta-llama/Llama-2-70b-hf ./models/llama2-70b

# Convert to ONNX format (if needed)
python scripts/convert_to_onnx.py \
  --model-path ./models/llama2-70b \
  --output-path ./models/llama2-70b-onnx \
  --precision fp16
```

### Step 2: Create Model Sharding Configuration

```yaml
# models/llama2-70b-config.yaml
model:
  name: "LLaMA-2-70B"
  format: "onnx"
  total_parameters: 70000000000
  total_layers: 80
  vocab_size: 32000
  hidden_size: 8192
  
sharding:
  strategy: "layer_wise"
  shards_count: 8
  layers_per_shard: 10
  overlap_layers: 0
  
memory:
  max_shard_memory_gb: 40
  enable_quantization: true
  quantization_bits: 16
  
optimization:
  enable_kv_cache: true
  max_sequence_length: 4096
  batch_size: 4
```

### Step 3: Upload Models to Azure Storage

```bash
# Get storage account key
STORAGE_KEY=$(az storage account keys list \
  --resource-group $RESOURCE_GROUP \
  --account-name $STORAGE_ACCOUNT \
  --query '[0].value' --output tsv)

# Create container
az storage container create \
  --name models \
  --account-name $STORAGE_ACCOUNT \
  --account-key $STORAGE_KEY

# Upload model files (this may take several hours for large models)
az storage blob upload-batch \
  --destination models \
  --source ./models/llama2-70b-onnx \
  --account-name $STORAGE_ACCOUNT \
  --account-key $STORAGE_KEY \
  --pattern "*.onnx" \
  --max-connections 10
```

## ‚ò∏Ô∏è AKS Cluster Configuration

### Step 1: Create AKS Cluster with GPU Node Pools

```bash
# Get subnet ID
SUBNET_ID=$(az network vnet subnet show \
  --resource-group $RESOURCE_GROUP \
  --vnet-name zig-ai-vnet \
  --name aks-subnet \
  --query id --output tsv)

# Create AKS cluster
az aks create \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER_NAME \
  --location $LOCATION \
  --kubernetes-version 1.28.0 \
  --node-count 1 \
  --node-vm-size Standard_D4s_v3 \
  --vnet-subnet-id $SUBNET_ID \
  --docker-bridge-address 172.17.0.1/16 \
  --dns-service-ip 10.2.0.10 \
  --service-cidr 10.2.0.0/24 \
  --enable-addons monitoring \
  --enable-cluster-autoscaler \
  --min-count 1 \
  --max-count 3 \
  --generate-ssh-keys

# Add GPU node pool for inference shards
az aks nodepool add \
  --resource-group $RESOURCE_GROUP \
  --cluster-name $AKS_CLUSTER_NAME \
  --name gpupool \
  --node-count 4 \
  --node-vm-size Standard_NC24s_v3 \
  --enable-cluster-autoscaler \
  --min-count 2 \
  --max-count 8 \
  --node-taints nvidia.com/gpu=true:NoSchedule \
  --labels accelerator=nvidia-tesla-v100
```

### Step 2: Configure Cluster Access and GPU Support

```bash
# Get cluster credentials
az aks get-credentials \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER_NAME

# Install NVIDIA device plugin
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.1/nvidia-device-plugin.yml

# Verify GPU nodes
kubectl get nodes -l accelerator=nvidia-tesla-v100
kubectl describe node <gpu-node-name> | grep nvidia.com/gpu

## üöÄ Deployment Steps

### Step 1: Build and Push Container Images

```bash
# Clone the repository
git clone https://github.com/anachary/zig-ai-platform.git
cd zig-ai-platform

# Login to ACR
az acr login --name $ACR_NAME

# Build coordinator image
docker build -f deploy/aks/Dockerfile.coordinator \
  -t $ACR_NAME.azurecr.io/zig-ai/coordinator:v1.0.0 .

# Build shard image
docker build -f deploy/aks/Dockerfile.shard \
  -t $ACR_NAME.azurecr.io/zig-ai/shard:v1.0.0 .

# Push images
docker push $ACR_NAME.azurecr.io/zig-ai/coordinator:v1.0.0
docker push $ACR_NAME.azurecr.io/zig-ai/shard:v1.0.0
```

### Step 2: Create Kubernetes Secrets and ConfigMaps

```bash
# Create namespace
kubectl create namespace zig-ai

# Create secret for Azure Storage
kubectl create secret generic azure-storage-secret \
  --namespace zig-ai \
  --from-literal=account-name=$STORAGE_ACCOUNT \
  --from-literal=account-key=$STORAGE_KEY

# Create ConfigMap for model configuration
kubectl create configmap model-config \
  --namespace zig-ai \
  --from-file=models/llama2-70b-config.yaml

# Create secret for ACR access
kubectl create secret docker-registry acr-secret \
  --namespace zig-ai \
  --docker-server=$ACR_NAME.azurecr.io \
  --docker-username=$ACR_NAME \
  --docker-password=$(az acr credential show --name $ACR_NAME --query passwords[0].value --output tsv)
```

### Step 3: Deploy Storage Resources

```bash
# Apply storage configurations
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-storage-pvc
  namespace: zig-ai
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: azurefile-premium
  resources:
    requests:
      storage: 2Ti
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cache-storage-pvc
  namespace: zig-ai
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: managed-premium
  resources:
    requests:
      storage: 500Gi
EOF
```

### Step 4: Deploy with Helm

```bash
# Create custom values file for massive LLM deployment
cat <<EOF > values-llm-production.yaml
global:
  imageRegistry: "$ACR_NAME.azurecr.io"
  imagePullSecrets:
    - name: acr-secret

coordinator:
  replicas: 1
  image:
    repository: "zig-ai/coordinator"
    tag: "v1.0.0"
  resources:
    requests:
      memory: "16Gi"
      cpu: "8"
    limits:
      memory: "32Gi"
      cpu: "16"
  config:
    shardsCount: 8
    maxConnections: 2000
    replicationFactor: 1

shard:
  replicas: 8
  image:
    repository: "zig-ai/shard"
    tag: "v1.0.0"
  resources:
    requests:
      memory: "48Gi"
      cpu: "16"
      nvidia.com/gpu: 1
    limits:
      memory: "64Gi"
      cpu: "24"
      nvidia.com/gpu: 1
  config:
    maxMemoryGb: 48
    workerThreads: 24
    enableGpu: true
  nodeSelector:
    accelerator: nvidia-tesla-v100
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

storage:
  modelStorage:
    enabled: true
    size: "2Ti"
    storageClass: "azurefile-premium"
  shardStorage:
    enabled: true
    size: "500Gi"
    storageClass: "managed-premium"

autoscaling:
  enabled: true
  minReplicas: 4
  maxReplicas: 16
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

monitoring:
  enabled: true
  serviceMonitor:
    enabled: true

azure:
  acr:
    enabled: true
    name: "$ACR_NAME"
  storage:
    enabled: true
    accountName: "$STORAGE_ACCOUNT"
    containerName: "models"
EOF

# Deploy with Helm
helm install zig-ai-llm ./deploy/aks/helm/zig-ai-aks \
  --namespace zig-ai \
  --values values-llm-production.yaml \
  --wait --timeout=30m
```

### Step 5: Verify Deployment

```bash
# Check pod status
kubectl get pods -n zig-ai -w

# Check GPU allocation
kubectl describe nodes -l accelerator=nvidia-tesla-v100 | grep -A 5 "Allocated resources"

# Check logs
kubectl logs -n zig-ai deployment/zig-ai-coordinator -f
kubectl logs -n zig-ai deployment/zig-ai-shard -f

# Test inference endpoint
kubectl port-forward -n zig-ai service/zig-ai-coordinator-service 8080:8080 &
curl -X POST http://localhost:8080/v1/inference \
  -H "Content-Type: application/json" \
  -d '{"prompt": "The future of AI is", "max_tokens": 100}'
```

## üìà Scaling and Optimization

### Horizontal Pod Autoscaler Configuration

```bash
# Apply advanced HPA configuration
cat <<EOF | kubectl apply -f -
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: zig-ai-shard-hpa
  namespace: zig-ai
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: zig-ai-shard
  minReplicas: 4
  maxReplicas: 16
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: nvidia_gpu_utilization
      target:
        type: AverageValue
        averageValue: "80"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 600
      policies:
      - type: Percent
        value: 25
        periodSeconds: 60
EOF
```

### Cluster Autoscaler Configuration

```bash
# Configure cluster autoscaler for GPU nodes
kubectl patch deployment cluster-autoscaler \
  -n kube-system \
  -p '{"spec":{"template":{"spec":{"containers":[{"name":"cluster-autoscaler","command":["./cluster-autoscaler","--v=4","--stderrthreshold=info","--cloud-provider=azure","--skip-nodes-with-local-storage=false","--expander=least-waste","--node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/'$AKS_CLUSTER_NAME'","--balance-similar-node-groups","--scale-down-delay-after-add=10m","--scale-down-unneeded-time=10m","--max-node-provision-time=15m"]}]}}}}'
```

### Performance Optimization

```bash
# Apply performance optimizations
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: performance-config
  namespace: zig-ai
data:
  optimization.yaml: |
    performance:
      # GPU optimizations
      gpu:
        memory_fraction: 0.95
        allow_growth: true
        enable_mixed_precision: true

      # CPU optimizations
      cpu:
        numa_affinity: true
        thread_pool_size: 24
        enable_mkl: true

      # Memory optimizations
      memory:
        enable_memory_mapping: true
        prefetch_factor: 2
        cache_size_gb: 16

      # Network optimizations
      network:
        tcp_nodelay: true
        socket_buffer_size: 65536
        connection_pool_size: 100
EOF

## üìä Monitoring and Troubleshooting

### Step 1: Deploy Monitoring Stack

```bash
# Add Prometheus and Grafana repositories
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

# Install Prometheus
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
  --set grafana.adminPassword=admin123

# Install GPU monitoring
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/dcgm-exporter/main/dcgm-exporter.yaml
```

### Step 2: Configure Custom Dashboards

```bash
# Create Grafana dashboard for LLM metrics
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  llm-metrics.json: |
    {
      "dashboard": {
        "title": "Zig AI LLM Metrics",
        "panels": [
          {
            "title": "Inference Requests/sec",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(zig_ai_inference_requests_total[5m])"
              }
            ]
          },
          {
            "title": "GPU Utilization",
            "type": "graph",
            "targets": [
              {
                "expr": "DCGM_FI_DEV_GPU_UTIL"
              }
            ]
          },
          {
            "title": "Memory Usage per Shard",
            "type": "graph",
            "targets": [
              {
                "expr": "container_memory_usage_bytes{pod=~\"zig-ai-shard.*\"}"
              }
            ]
          },
          {
            "title": "Model Loading Time",
            "type": "stat",
            "targets": [
              {
                "expr": "zig_ai_model_load_duration_seconds"
              }
            ]
          }
        ]
      }
    }
EOF
```

### Step 3: Set Up Alerts

```bash
# Create alerting rules
cat <<EOF | kubectl apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: zig-ai-alerts
  namespace: zig-ai
spec:
  groups:
  - name: zig-ai.rules
    rules:
    - alert: HighGPUUtilization
      expr: DCGM_FI_DEV_GPU_UTIL > 90
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High GPU utilization detected"
        description: "GPU utilization is above 90% for more than 5 minutes"

    - alert: ShardDown
      expr: up{job="zig-ai-shard"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Shard instance is down"
        description: "Shard {{ \$labels.instance }} has been down for more than 1 minute"

    - alert: HighMemoryUsage
      expr: (container_memory_usage_bytes{pod=~"zig-ai-shard.*"} / container_spec_memory_limit_bytes) > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage in shard"
        description: "Memory usage is above 90% in shard {{ \$labels.pod }}"

    - alert: SlowInference
      expr: histogram_quantile(0.95, rate(zig_ai_inference_duration_seconds_bucket[5m])) > 10
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Slow inference response time"
        description: "95th percentile inference time is above 10 seconds"
EOF
```

### Common Troubleshooting Commands

```bash
# Check cluster resources
kubectl top nodes
kubectl top pods -n zig-ai

# Debug pod issues
kubectl describe pod <pod-name> -n zig-ai
kubectl logs <pod-name> -n zig-ai --previous

# Check GPU allocation
kubectl get nodes -o json | jq '.items[] | {name: .metadata.name, gpu: .status.allocatable."nvidia.com/gpu"}'

# Debug networking
kubectl exec -it <coordinator-pod> -n zig-ai -- netstat -tlnp
kubectl exec -it <shard-pod> -n zig-ai -- ping <coordinator-service>

# Check storage
kubectl get pv,pvc -n zig-ai
kubectl describe pvc model-storage-pvc -n zig-ai

# Performance debugging
kubectl exec -it <shard-pod> -n zig-ai -- nvidia-smi
kubectl exec -it <shard-pod> -n zig-ai -- top -p $(pgrep zig-ai)
```

## üí∞ Cost Optimization

### Step 1: Implement Spot Instances

```bash
# Create spot instance node pool
az aks nodepool add \
  --resource-group $RESOURCE_GROUP \
  --cluster-name $AKS_CLUSTER_NAME \
  --name spotgpupool \
  --priority Spot \
  --eviction-policy Delete \
  --spot-max-price 0.5 \
  --node-count 2 \
  --node-vm-size Standard_NC24s_v3 \
  --enable-cluster-autoscaler \
  --min-count 0 \
  --max-count 6 \
  --node-taints kubernetes.azure.com/scalesetpriority=spot:NoSchedule \
  --labels spot=true,accelerator=nvidia-tesla-v100
```

### Step 2: Configure Pod Disruption Budget

```bash
# Create PDB for graceful spot instance handling
cat <<EOF | kubectl apply -f -
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: zig-ai-shard-pdb
  namespace: zig-ai
spec:
  minAvailable: 50%
  selector:
    matchLabels:
      app.kubernetes.io/name: zig-ai-shard
EOF
```

### Step 3: Implement Auto-shutdown

```bash
# Create CronJob for auto-shutdown during off-hours
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: CronJob
metadata:
  name: auto-shutdown
  namespace: zig-ai
spec:
  schedule: "0 22 * * 1-5"  # 10 PM on weekdays
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: shutdown
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - |
              kubectl scale deployment zig-ai-shard --replicas=0 -n zig-ai
              kubectl scale deployment zig-ai-coordinator --replicas=0 -n zig-ai
          restartPolicy: OnFailure
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: auto-startup
  namespace: zig-ai
spec:
  schedule: "0 8 * * 1-5"   # 8 AM on weekdays
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: startup
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - |
              kubectl scale deployment zig-ai-coordinator --replicas=1 -n zig-ai
              sleep 60
              kubectl scale deployment zig-ai-shard --replicas=8 -n zig-ai
          restartPolicy: OnFailure
EOF
```

### Cost Monitoring

```bash
# Install Azure Cost Management exporter
helm repo add azure-cost-exporter https://github.com/Azure/azure-cost-exporter
helm install azure-cost-exporter azure-cost-exporter/azure-cost-exporter \
  --namespace monitoring \
  --set azure.subscriptionId=$SUBSCRIPTION_ID \
  --set azure.tenantId=$TENANT_ID

## üè≠ Production Considerations

### Security Best Practices

```bash
# Enable Pod Security Standards
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: zig-ai
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
EOF

# Create Network Policies
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: zig-ai-network-policy
  namespace: zig-ai
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: zig-ai
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: zig-ai
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: zig-ai
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: zig-ai
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: zig-ai
  - to: []
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
EOF

# Enable Azure Key Vault integration
az aks enable-addons \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER_NAME \
  --addons azure-keyvault-secrets-provider
```

### Backup and Disaster Recovery

```bash
# Create backup strategy for model data
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: CronJob
metadata:
  name: model-backup
  namespace: zig-ai
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: mcr.microsoft.com/azure-cli:latest
            env:
            - name: AZURE_STORAGE_ACCOUNT
              value: "$STORAGE_ACCOUNT"
            - name: AZURE_STORAGE_KEY
              valueFrom:
                secretKeyRef:
                  name: azure-storage-secret
                  key: account-key
            command:
            - /bin/sh
            - -c
            - |
              az storage blob sync \
                --source /data/models \
                --container backup-models \
                --account-name $AZURE_STORAGE_ACCOUNT \
                --account-key $AZURE_STORAGE_KEY
            volumeMounts:
            - name: model-storage
              mountPath: /data/models
          volumes:
          - name: model-storage
            persistentVolumeClaim:
              claimName: model-storage-pvc
          restartPolicy: OnFailure
EOF

# Create disaster recovery runbook
cat <<EOF > disaster-recovery-runbook.md
# Disaster Recovery Runbook

## Scenario 1: Complete Cluster Failure
1. Create new AKS cluster in different region
2. Restore model data from backup storage
3. Deploy application using Helm charts
4. Update DNS to point to new cluster

## Scenario 2: GPU Node Pool Failure
1. Scale down affected node pool
2. Create new GPU node pool
3. Drain and delete failed nodes
4. Verify shard redistribution

## Scenario 3: Storage Failure
1. Restore from Azure Storage backup
2. Create new PVCs
3. Restart affected pods
4. Verify model loading

## Recovery Time Objectives
- RTO: 4 hours for complete cluster recovery
- RPO: 24 hours for model data
- RTO: 30 minutes for node pool recovery
EOF
```

### Performance Tuning

```bash
# Apply advanced performance configurations
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: advanced-performance-config
  namespace: zig-ai
data:
  performance.conf: |
    # GPU Performance Settings
    export CUDA_VISIBLE_DEVICES=0
    export CUDA_DEVICE_ORDER=PCI_BUS_ID
    export CUDA_CACHE_PATH=/tmp/cuda-cache

    # Memory Management
    export MALLOC_ARENA_MAX=4
    export MALLOC_MMAP_THRESHOLD_=131072
    export MALLOC_TRIM_THRESHOLD_=131072

    # Network Optimization
    echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
    echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
    echo 'net.ipv4.tcp_rmem = 4096 87380 134217728' >> /etc/sysctl.conf
    echo 'net.ipv4.tcp_wmem = 4096 65536 134217728' >> /etc/sysctl.conf

    # CPU Optimization
    echo 'performance' > /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
EOF
```

### Load Testing

```bash
# Create load testing job
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: load-test
  namespace: zig-ai
spec:
  parallelism: 10
  template:
    spec:
      containers:
      - name: load-test
        image: curlimages/curl:latest
        command:
        - /bin/sh
        - -c
        - |
          for i in \$(seq 1 1000); do
            curl -X POST http://zig-ai-coordinator-service:8080/v1/inference \
              -H "Content-Type: application/json" \
              -d '{"prompt": "Test prompt \$i", "max_tokens": 50}' \
              --max-time 30 || echo "Request \$i failed"
            sleep 0.1
          done
      restartPolicy: Never
  backoffLimit: 3
EOF

# Monitor load test results
kubectl logs -f job/load-test -n zig-ai
```

## üìã Deployment Checklist

### Pre-deployment Checklist
- [ ] Azure subscription with sufficient GPU quota
- [ ] Model files prepared and uploaded to Azure Storage
- [ ] AKS cluster created with GPU node pools
- [ ] Container images built and pushed to ACR
- [ ] Kubernetes secrets and configmaps created
- [ ] Storage classes and PVCs configured
- [ ] Network policies and security configurations applied

### Post-deployment Checklist
- [ ] All pods running and healthy
- [ ] GPU resources properly allocated
- [ ] Model loading completed successfully
- [ ] Inference endpoints responding correctly
- [ ] Monitoring and alerting configured
- [ ] Autoscaling policies tested
- [ ] Backup procedures verified
- [ ] Load testing completed
- [ ] Documentation updated

### Production Readiness Checklist
- [ ] Security scanning completed
- [ ] Performance benchmarks established
- [ ] Disaster recovery plan tested
- [ ] Cost optimization measures implemented
- [ ] Team training completed
- [ ] Operational runbooks created
- [ ] SLA/SLO definitions established
- [ ] Compliance requirements met

## üîó Additional Resources

### Documentation Links
- [Azure Kubernetes Service Documentation](https://docs.microsoft.com/en-us/azure/aks/)
- [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html)
- [Kubernetes GPU Scheduling](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)
- [Helm Charts Best Practices](https://helm.sh/docs/chart_best_practices/)

### Monitoring and Observability
- [Prometheus Operator](https://prometheus-operator.dev/)
- [Grafana Dashboards](https://grafana.com/grafana/dashboards/)
- [Azure Monitor for Containers](https://docs.microsoft.com/en-us/azure/azure-monitor/containers/)

### Cost Management
- [Azure Cost Management](https://docs.microsoft.com/en-us/azure/cost-management-billing/)
- [Spot Instance Best Practices](https://docs.microsoft.com/en-us/azure/virtual-machines/spot-vms)
- [AKS Cost Optimization](https://docs.microsoft.com/en-us/azure/aks/best-practices-cost)

---

## üéØ Summary

This guide provides a comprehensive approach to deploying massive pretrained LLMs on Azure Kubernetes Service. The distributed inference architecture enables efficient scaling and cost-effective operation of large language models in production environments.

**Key Benefits:**
- ‚úÖ **Scalable**: Handle models up to 540B+ parameters
- ‚úÖ **Cost-effective**: Spot instances and auto-scaling reduce costs by 60-80%
- ‚úÖ **Reliable**: Fault tolerance and disaster recovery ensure 99.9% uptime
- ‚úÖ **Secure**: Enterprise-grade security with network policies and encryption
- ‚úÖ **Observable**: Comprehensive monitoring and alerting

For additional support or questions, please refer to the [Contributing Guide](../CONTRIBUTING.md) or open an issue in the repository.
```
```
```
