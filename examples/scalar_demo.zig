const std = @import("std");
const lib = @import("zig-ai-inference");

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    std.log.info("ðŸ”¢ 0D Scalar Tensor Demo - Zig AI Inference Engine", .{});
    std.log.info("=================================================", .{});

    // Initialize the AI engine
    var engine = try lib.Engine.init(allocator, .{
        .max_memory_mb = 256,
        .num_threads = 1,
        .enable_profiling = true,
    });
    defer engine.deinit();

    std.log.info("âœ… Engine initialized", .{});
    std.log.info("", .{});

    // Demo 1: Create 0D scalar tensors
    std.log.info("ðŸ“Š Demo 1: Creating 0D Scalar Tensors", .{});
    std.log.info("=====================================", .{});

    // Method 1: Using init with empty shape
    var scalar1 = try lib.tensor.Tensor.init(allocator, &[_]usize{}, .f32);
    defer scalar1.deinit();
    try scalar1.set_scalar_f32(3.14159);

    std.log.info("Scalar 1 (Ï€): {d:.5}", .{try scalar1.get_scalar_f32()});
    std.log.info("  - Dimensions: {d}D", .{scalar1.ndim()});
    std.log.info("  - Elements: {d}", .{scalar1.numel()});
    std.log.info("  - Is scalar: {}", .{scalar1.is_scalar()});

    // Method 2: Using convenience function
    var scalar2 = try lib.tensor.Tensor.scalar(allocator, 42.0, .f32);
    defer scalar2.deinit();

    std.log.info("Scalar 2: {d}", .{try scalar2.get_scalar_f32()});
    std.log.info("  - Dimensions: {d}D", .{scalar2.ndim()});
    std.log.info("  - Elements: {d}", .{scalar2.numel()});

    // Method 3: Integer scalar
    var int_scalar = try lib.tensor.Tensor.scalar(allocator, 100, .i32);
    defer int_scalar.deinit();

    const int_data = @as([*]const i32, @ptrCast(@alignCast(int_scalar.data.ptr)));
    std.log.info("Integer scalar: {d}", .{int_data[0]});

    std.log.info("", .{});

    // Demo 2: Scalar operations with engine
    std.log.info("ðŸ”§ Demo 2: Scalar Operations", .{});
    std.log.info("============================", .{});

    // Get scalars from engine tensor pool
    var engine_scalar1 = try engine.get_tensor(&[_]usize{}, .f32);
    defer engine.return_tensor(engine_scalar1) catch {};

    var engine_scalar2 = try engine.get_tensor(&[_]usize{}, .f32);
    defer engine.return_tensor(engine_scalar2) catch {};

    var result_scalar = try engine.get_tensor(&[_]usize{}, .f32);
    defer engine.return_tensor(result_scalar) catch {};

    // Set values
    try engine_scalar1.set_scalar_f32(10.5);
    try engine_scalar2.set_scalar_f32(2.5);

    std.log.info("Input scalar A: {d}", .{try engine_scalar1.get_scalar_f32()});
    std.log.info("Input scalar B: {d}", .{try engine_scalar2.get_scalar_f32()});

    // Perform scalar addition using engine operators
    const add_inputs = [_]lib.Tensor{ engine_scalar1, engine_scalar2 };
    var add_outputs = [_]lib.Tensor{result_scalar};
    try engine.execute_operator("Add", &add_inputs, &add_outputs);

    std.log.info("Result (A + B): {d}", .{try result_scalar.get_scalar_f32()});

    std.log.info("", .{});

    // Demo 3: Scalar to vector conversion
    std.log.info("ðŸ”„ Demo 3: Scalar â†” Vector Conversion", .{});
    std.log.info("====================================", .{});

    // Create a scalar
    var original_scalar = try lib.tensor.Tensor.scalar(allocator, 7.5, .f32);
    defer original_scalar.deinit();

    std.log.info("Original scalar: {d} (shape: [])", .{try original_scalar.get_scalar_f32()});

    // Reshape to 1D vector with 1 element
    var as_vector = try original_scalar.reshape(allocator, &[_]usize{1});
    defer as_vector.deinit();

    std.log.info("As 1D vector: {d} (shape: [1])", .{try as_vector.get_f32(&[_]usize{0})});
    std.log.info("  - Dimensions: {d}D", .{as_vector.ndim()});
    std.log.info("  - Elements: {d}", .{as_vector.numel()});

    // Reshape back to scalar
    var back_to_scalar = try as_vector.reshape(allocator, &[_]usize{});
    defer back_to_scalar.deinit();

    std.log.info("Back to scalar: {d} (shape: [])", .{try back_to_scalar.get_scalar_f32()});

    std.log.info("", .{});

    // Demo 4: Broadcasting with scalars
    std.log.info("ðŸ“¡ Demo 4: Broadcasting with Scalars", .{});
    std.log.info("===================================", .{});

    // Create a 2x3 matrix
    var matrix = try engine.get_tensor(&[_]usize{ 2, 3 }, .f32);
    defer engine.return_tensor(matrix) catch {};

    // Fill matrix with values
    try matrix.set_f32(&[_]usize{ 0, 0 }, 1.0);
    try matrix.set_f32(&[_]usize{ 0, 1 }, 2.0);
    try matrix.set_f32(&[_]usize{ 0, 2 }, 3.0);
    try matrix.set_f32(&[_]usize{ 1, 0 }, 4.0);
    try matrix.set_f32(&[_]usize{ 1, 1 }, 5.0);
    try matrix.set_f32(&[_]usize{ 1, 2 }, 6.0);

    std.log.info("Matrix [2x3]:", .{});
    std.log.info("  [{d:.1}, {d:.1}, {d:.1}]", .{ try matrix.get_f32(&[_]usize{ 0, 0 }), try matrix.get_f32(&[_]usize{ 0, 1 }), try matrix.get_f32(&[_]usize{ 0, 2 }) });
    std.log.info("  [{d:.1}, {d:.1}, {d:.1}]", .{ try matrix.get_f32(&[_]usize{ 1, 0 }), try matrix.get_f32(&[_]usize{ 1, 1 }), try matrix.get_f32(&[_]usize{ 1, 2 }) });

    // Create a scalar for broadcasting
    var broadcast_scalar = try engine.get_tensor(&[_]usize{}, .f32);
    defer engine.return_tensor(broadcast_scalar) catch {};
    try broadcast_scalar.set_scalar_f32(10.0);

    std.log.info("Scalar for broadcasting: {d}", .{try broadcast_scalar.get_scalar_f32()});

    // Note: Full broadcasting would require additional implementation
    // This demonstrates the scalar creation and access patterns
    std.log.info("âœ… Scalar broadcasting patterns demonstrated", .{});

    std.log.info("", .{});

    // Demo 5: Performance comparison
    std.log.info("âš¡ Demo 5: Performance Characteristics", .{});
    std.log.info("====================================", .{});

    const iterations = 100000;
    var timer = try std.time.Timer.start();

    // Scalar operations
    timer.reset();
    for (0..iterations) |_| {
        var perf_scalar = try lib.tensor.Tensor.scalar(allocator, 1.0, .f32);
        try perf_scalar.set_scalar_f32(try perf_scalar.get_scalar_f32() + 1.0);
        perf_scalar.deinit();
    }
    const scalar_time = timer.read();

    // Vector operations for comparison
    timer.reset();
    for (0..iterations) |_| {
        var perf_vector = try lib.tensor.Tensor.init(allocator, &[_]usize{1}, .f32);
        try perf_vector.set_f32(&[_]usize{0}, try perf_vector.get_f32(&[_]usize{0}) + 1.0);
        perf_vector.deinit();
    }
    const vector_time = timer.read();

    std.log.info("Performance ({d} iterations):", .{iterations});
    std.log.info("  Scalar ops: {d:.2}ms", .{@as(f64, @floatFromInt(scalar_time)) / 1_000_000.0});
    std.log.info("  Vector ops: {d:.2}ms", .{@as(f64, @floatFromInt(vector_time)) / 1_000_000.0});
    std.log.info("  Scalar efficiency: {d:.1}x", .{@as(f64, @floatFromInt(vector_time)) / @as(f64, @floatFromInt(scalar_time))});

    std.log.info("", .{});
    std.log.info("ðŸŽ‰ 0D Scalar Demo Complete!", .{});
    std.log.info("âœ… Your Zig AI Engine now supports:", .{});
    std.log.info("   â€¢ 0D scalar tensors", .{});
    std.log.info("   â€¢ Scalar-specific operations", .{});
    std.log.info("   â€¢ Scalar â†” tensor conversions", .{});
    std.log.info("   â€¢ Broadcasting foundations", .{});
    std.log.info("   â€¢ Memory-efficient scalar storage", .{});
}
