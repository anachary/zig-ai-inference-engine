# Zig AI Platform

A zero-dependency AI inference library for all model formats. Built as a DLL with real GGUF model support.

## What is this

The zig-ai-platform is a production-ready AI inference library that loads and runs models from multiple formats - starting with GGUF, expanding to ONNX, SafeTensors, PyTorch, and more. Unlike other libraries, it's built as a **dynamic library (DLL)** that applications can consume, with zero dependencies beyond the Zig standard library.

**Key Features:**
- ğŸš€ **Real GGUF model loading** with actual tensor data access
- ğŸ“¦ **DLL architecture** - library + applications (CLI, etc.)
- ğŸ” **Smart tensor organization** - automatically structures transformer weights
- ğŸ’¾ **Memory efficient** - explicit allocations, no hidden malloc calls
- ğŸ¯ **Format detection** - automatic model format identification
- âš¡ **Performance focused** - SIMD optimizations and zero-cost abstractions

## Why Zig

Zig gives us the performance of C with the safety of modern languages, plus unique advantages for AI workloads:

- **Comptime computation** - optimize models at build time
- **Explicit memory management** - no hidden allocations in inference loops
- **Cross compilation** - works out of the box for web, mobile, embedded
- **Zero cost abstractions** - clean APIs without performance penalties
- **No dependencies** - pure Zig standard library only

## Current Status

**ğŸ‰ MAJOR MILESTONE ACHIEVED**: Real GGUF model loading and CLI interface working!

**âœ… What's Working:**
- Real GGUF model loading (379MB+ models)
- Tensor data access and organization
- CLI interface with format detection and chat
- DLL compilation and export
- Model metadata parsing
- Transformer inference framework

**ğŸš§ In Progress:**
- Quantization support (Q4_K_M, F16, Q8_0)
- Real mathematical operations integration
- Actual attention and feed-forward computation

## Implementation Roadmap

### âœ… Phase 1: Foundation (COMPLETED)
**Status**: **DONE** - Core infrastructure is solid

**Achievements:**
- âœ… Core tensor abstraction with dynamic shapes
- âœ… Universal Model, Tokenizer, Inference interfaces
- âœ… Memory management with explicit allocators
- âœ… Error handling with clear error types
- âœ… Build system with DLL support
- âœ… Cross-compilation ready

### âœ… Phase 2: GGUF Foundation (COMPLETED)
**Status**: **DONE** - Real GGUF models loading successfully

**Achievements:**
- âœ… Complete GGUF parser with real tensor loading
- âœ… Model metadata extraction from GGUF files
- âœ… Tensor organization into transformer structure
- âœ… Format detection and validation
- âœ… CLI interface with real model support

**Demo Working:**
```bash
# Real model detection
zig build run -- detect models/Qwen2-0.5B-Instruct-Q4_K_M.gguf

# Interactive chat with real models
zig build run -- chat models/Qwen2-0.5B-Instruct-Q4_K_M.gguf
```

### ğŸš§ Phase 3: Real Inference (IN PROGRESS)
**Status**: **ACTIVE** - Implementing actual mathematical operations

**Critical Path to Real Responses:**

#### **Week 1: Quantization Support (HIGHEST PRIORITY)**
- ğŸ”§ **Q4_K_M dequantization** - Unlock Qwen2/Llama-2 weights
- ğŸ”§ **F16 dequantization** - Common format support
- ğŸ”§ **Q8_0 dequantization** - Backup format

#### **Week 2: Core Operations Integration**
- ğŸ§® **Real token embedding** with dequantization
- ğŸ§® **Matrix multiplication** integration with transformer
- ğŸ§® **Layer normalization** integration

#### **Week 3: Multi-Head Attention**
- ğŸ” **Scaled dot-product attention** implementation
- ğŸ” **Multi-head reshaping** and concatenation
- ğŸ” **Causal masking** for autoregressive generation
- ğŸ” **Rotary Position Embedding (RoPE)** if needed

#### **Week 4: Feed-Forward Networks**
- ğŸ”„ **SwiGLU activation** (SiLU + gating)
- ğŸ”„ **Feed-forward layers** with residual connections
- ğŸ”„ **Output projection** with proper matrix ops

#### **Week 5: Optimization & Polish**
- âš¡ **KV caching** for efficient generation
- âš¡ **SIMD optimizations** for performance
- âš¡ **Memory management** optimization

**Target**: **Real AI responses** from actual model weights by end of Phase 3

### ğŸ“‹ Phase 4: Advanced Features (PLANNED)
**Status**: **PLANNED** - After real inference works

**Tokenization Enhancement:**
- BPE tokenizer with real vocabulary loading
- Special token handling
- Efficient encoding/decoding

**Performance Optimization:**
- SIMD-optimized matrix operations
- Parallel computation support
- Memory pool optimization

### ğŸŒŸ Phase 5: Format Expansion (FUTURE)
**Status**: **FUTURE** - Multi-format support

**Format Priority:**
1. **ONNX** - Industry standard for deployment
2. **SafeTensors** - Modern safe format
3. **PyTorch** - Research and development
4. **TensorFlow** - Enterprise deployment

### ğŸš€ Phase 6: Production Ready (FUTURE)
**Status**: **FUTURE** - Enterprise deployment

**Hardware Backends:**
- CPU optimization (baseline)
- WASM for web deployment
- GPU compute shaders
- Mobile/embedded targets

## Current Architecture

### **DLL + Applications Structure**
```
zig-ai-platform/
â”œâ”€â”€ ğŸ“¦ zig-out/lib/zig-ai-platform.dll    # Core AI library
â”œâ”€â”€ ğŸ–¥ï¸ zig-out/bin/zig-ai-cli.exe         # CLI application
â”œâ”€â”€ ğŸ“ models/                            # Real GGUF models
â”‚   â”œâ”€â”€ Qwen2-0.5B-Instruct-Q4_K_M.gguf  # 379 MB
â”‚   â””â”€â”€ llama-2-7b-chat.gguf              # 3.2 GB
â””â”€â”€ ğŸ“ examples/                          # Example applications
```

### **Real Model Loading Pipeline**
```zig
// 1. Load real GGUF model
var model = try zig_ai.loadModel(allocator, "model.gguf");

// 2. Access organized tensors
const embeddings = model.getTensor("token_embd.weight");
const layer0_attn = model.getTensor("blk.0.attn_q.weight");

// 3. Run real transformer inference
var transformer = try RealTransformer.init(allocator, &model);
const logits = try transformer.forward(tokens, &context);

// 4. Sample next token
const next_token = try transformer.sample(logits, &config);
```

## Memory Management Philosophy

**Everything is explicit. No hidden allocations:**

```zig
// Always explicit about memory
var gpa = std.heap.GeneralPurposeAllocator(.{}){};
defer _ = gpa.deinit();
const allocator = gpa.allocator();

// Load model with explicit allocator
var model = try zig_ai.loadModel(allocator, "model.gguf");
defer model.deinit();

// No hidden allocations in inference
const result = try transformer.forward(tokens, &context);
defer allocator.free(result);
```

## Building & Usage

### **Build the Library and CLI**
```bash
zig build                    # Builds both DLL and CLI
```

### **Test with Real Models**
```bash
# Detect model format
zig build run -- detect models/Qwen2-0.5B-Instruct-Q4_K_M.gguf

# Start interactive chat
zig build run -- chat models/Qwen2-0.5B-Instruct-Q4_K_M.gguf
```

### **Use as Library**
```zig
// Link against zig-ai-platform.dll
const zig_ai = @import("zig-ai-platform");

// C API for other languages
extern fn zig_ai_get_version() [*:0]const u8;
extern fn zig_ai_detect_format(path: [*:0]const u8) c_int;
```

## Performance Targets

- **Zero allocations** in inference hot paths
- **Real tensor operations** with SIMD optimization
- **Explicit memory management** - user controls all allocations
- **Cross-platform** - same codebase for all targets
- **No runtime dependencies** beyond libc

## Next Immediate Steps

1. **Implement Q4_K_M dequantization** - Unlock real model weights
2. **Integrate matrix operations** - Connect math library to transformer
3. **Real attention computation** - Multi-head attention with actual weights
4. **Feed-forward networks** - SwiGLU activation and projections
5. **Output generation** - Real logits from model weights

**Goal**: Real AI responses from actual model weights within 4 weeks.

## Contributing

We're actively implementing real inference! Priority areas:
- **Quantization algorithms** (Q4_K_M, F16, Q8_0)
- **Mathematical operations** integration
- **Performance optimization** with SIMD
- **Testing** with real models

## License

MIT

